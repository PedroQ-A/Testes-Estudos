<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FlowForge Dashboard LLM - PRD</title>
    <style>
        @media print {
            body { margin: 0; }
            .page-break { page-break-after: always; }
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: white;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-top: 40px;
            font-size: 2.5em;
        }
        
        h2 {
            color: #34495e;
            margin-top: 35px;
            font-size: 1.8em;
            border-bottom: 1px solid #ecf0f1;
            padding-bottom: 5px;
        }
        
        h3 {
            color: #7f8c8d;
            margin-top: 25px;
            font-size: 1.4em;
        }
        
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 5px;
            padding: 15px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
        }
        
        code {
            background: #f8f9fa;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        th {
            background-color: #3498db;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: bold;
        }
        
        td {
            padding: 10px 12px;
            border-bottom: 1px solid #ecf0f1;
        }
        
        tr:hover {
            background-color: #f8f9fa;
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        blockquote {
            border-left: 4px solid #3498db;
            padding-left: 20px;
            margin: 20px 0;
            color: #7f8c8d;
            font-style: italic;
        }
        
        .section {
            margin-bottom: 40px;
        }
        
        .highlight {
            background-color: #fff3cd;
            padding: 2px 4px;
            border-radius: 3px;
        }
        
        @media print {
            h1 { color: black; }
            h2 { color: #333; }
            h3 { color: #555; }
            th { background-color: #ddd; color: black; }
        }
    </style>
</head>
<body>
    <p><h1>Product Requirements Document (PRD)</h1>
<h1>FlowForge Dashboard com IntegraÃ§Ã£o LLM</h1></p><p><strong>VersÃ£o:</strong> 2.0.0  
<strong>Data:</strong> 2025-09-12  
<strong>Status:</strong> Para AprovaÃ§Ã£o  
<strong>Abordagem:</strong> HÃ­brida (LLM Local + Cloud)  
<strong>RevisÃ£o:</strong> Enhanced with comprehensive sections</p><p>---</p><p><h2>ğŸ“‹ SumÃ¡rio Executivo</h2></p><p><h3>VisÃ£o do Produto</h3>
O <strong>FlowForge Dashboard com IntegraÃ§Ã£o LLM</strong> Ã© uma plataforma de visualizaÃ§Ã£o gerencial que permite aos gestores monitorar em tempo real a produtividade de suas equipes atravÃ©s de conversas naturais com inteligÃªncia artificial. O sistema combina o poder de LLMs locais (custo zero) com LLMs cloud (anÃ¡lises complexas) para fornecer insights profundos sobre performance, gargalos e oportunidades de melhoria.</p><p><h3>Problema a Resolver</h3>
<ul><li><strong>Visibilidade limitada</strong>: Gestores nÃ£o tÃªm visÃ£o consolidada do trabalho da equipe</li></ul>
<ul><li><strong>AnÃ¡lise manual</strong>: Horas gastas criando relatÃ³rios e analisando dados</li></ul>
<ul><li><strong>Insights perdidos</strong>: PadrÃµes e anomalias nÃ£o detectados em tempo hÃ¡bil</li></ul>
<ul><li><strong>Custo de LLM</strong>: APIs cloud caras para queries simples e repetitivas</li></ul></p><p><h3>SoluÃ§Ã£o Proposta</h3>
Dashboard interativo com interface conversacional que permite queries em linguagem natural, gerando automaticamente visualizaÃ§Ãµes e insights atravÃ©s de uma arquitetura hÃ­brida que otimiza custos (70% de reduÃ§Ã£o) mantendo alta qualidade de resposta.</p><p><h3>BenefÃ­cios Principais</h3>
<ul><li><strong>Economia de 60-70%</strong> nos custos de LLM atravÃ©s de processamento hÃ­brido</li></ul>
<ul><li><strong>Respostas em <500ms</strong> para queries locais, <3s para cloud</li></ul>
<ul><li><strong>Interface conversacional</strong> em portuguÃªs com suporte a voz</li></ul>
<ul><li><strong>VisualizaÃ§Ãµes automÃ¡ticas</strong> baseadas no contexto da pergunta</li></ul>
<ul><li><strong>Insights proativos</strong> com detecÃ§Ã£o de anomalias e previsÃµes</li></ul></p><p>---</p><p><h2>ğŸ¯ Objetivos e Metas</h2></p><p><h3>Objetivos PrimÃ¡rios</h3>
1. <strong>Visibilidade Total</strong>: Dashboard unificado com mÃ©tricas em tempo real
2. <strong>AnÃ¡lise por IA</strong>: Queries em linguagem natural gerando insights automÃ¡ticos
3. <strong>ReduÃ§Ã£o de Custos</strong>: Minimizar gastos com APIs de LLM atravÃ©s de arquitetura hÃ­brida
4. <strong>Produtividade</strong>: Eliminar geraÃ§Ã£o manual de relatÃ³rios (economia de 10h/semana)
5. <strong>DecisÃµes Data-Driven</strong>: Insights acionÃ¡veis baseados em dados reais</p><p><h3>Metas SMART</h3>
<ul><li><strong>Q1 2025</strong>: MVP funcional com 5 tipos de queries prÃ©-definidas</li></ul>
<ul><li><strong>Q2 2025</strong>: IntegraÃ§Ã£o hÃ­brida completa (local + cloud) com 60% economia</li></ul>
<ul><li><strong>Q3 2025</strong>: Fine-tuning com dados reais, 90% accuracy em queries especÃ­ficas</li></ul>
<ul><li><strong>Q4 2025</strong>: 100% adoÃ§Ã£o pela equipe, ROI positivo demonstrado</li></ul></p><p><h3>KPIs de Sucesso</h3>
| MÃ©trica | Meta | MediÃ§Ã£o |
|---------|------|---------|
| <strong>Tempo de Resposta</strong> | <500ms local, <3s cloud | p95 latency |
| <strong>ReduÃ§Ã£o de Custos</strong> | 60-70% vs cloud-only | Custo mensal de API |
| <strong>SatisfaÃ§Ã£o do UsuÃ¡rio</strong> | >4.5/5 | NPS trimestral |
| <strong>Queries Processadas</strong> | >1000/dia | Analytics dashboard |
| <strong>Uptime</strong> | 99.9% | Monitoring tools |
| <strong>Adoption Rate</strong> | 100% em 3 meses | Active users |</p><p>---</p><p><h2>ğŸ“š GlossÃ¡rio TÃ©cnico</h2></p><p><h3>Termos de IA/LLM</h3>
<ul><li><strong>LLM (Large Language Model)</strong>: Modelo de linguagem grande treinado para processar e gerar texto</li></ul>
<ul><li><strong>RAG (Retrieval-Augmented Generation)</strong>: TÃ©cnica que combina recuperaÃ§Ã£o de informaÃ§Ãµes com geraÃ§Ã£o</li></ul>
<ul><li><strong>Embedding</strong>: RepresentaÃ§Ã£o vetorial de texto para comparaÃ§Ã£o semÃ¢ntica</li></ul>
<ul><li><strong>Fine-tuning</strong>: Ajuste de modelo prÃ©-treinado com dados especÃ­ficos</li></ul>
<ul><li><strong>Hallucination</strong>: Resposta incorreta ou inventada pelo LLM</li></ul>
<ul><li><strong>Token</strong>: Unidade bÃ¡sica de texto processada pelo LLM</li></ul>
<ul><li><strong>Prompt Injection</strong>: Ataque que manipula entrada para obter resposta indesejada</li></ul>
<ul><li><strong>Vector Store</strong>: Banco de dados especializado em busca por similaridade</li></ul>
<ul><li><strong>Semantic Cache</strong>: Cache baseado em significado, nÃ£o em texto exato</li></ul></p><p><h3>Termos Arquiteturais</h3>
<ul><li><strong>API Gateway</strong>: Ponto Ãºnico de entrada para requisiÃ§Ãµes da API</li></ul>
<ul><li><strong>Load Balancer</strong>: Distribuidor de carga entre mÃºltiplos servidores</li></ul>
<ul><li><strong>Circuit Breaker</strong>: PadrÃ£o que previne cascata de falhas</li></ul>
<ul><li><strong>Blue-Green Deploy</strong>: EstratÃ©gia de implantaÃ§Ã£o sem downtime</li></ul>
<ul><li><strong>Horizontal Scaling</strong>: AdiÃ§Ã£o de mais servidores para aumentar capacidade</li></ul>
<ul><li><strong>Microservices</strong>: Arquitetura de pequenos serviÃ§os independentes</li></ul>
<ul><li><strong>Event Sourcing</strong>: PadrÃ£o que armazena mudanÃ§as como eventos</li></ul>
<ul><li><strong>CQRS</strong>: SeparaÃ§Ã£o de comandos (write) e consultas (read)</li></ul></p><p><h3>Termos de NegÃ³cio</h3>
<ul><li><strong>ROI (Return on Investment)</strong>: Retorno sobre investimento</li></ul>
<ul><li><strong>NPS (Net Promoter Score)</strong>: MÃ©trica de satisfaÃ§Ã£o do cliente</li></ul>
<ul><li><strong>KPI (Key Performance Indicator)</strong>: Indicador-chave de performance</li></ul>
<ul><li><strong>SLA (Service Level Agreement)</strong>: Acordo de nÃ­vel de serviÃ§o</li></ul>
<ul><li><strong>TCO (Total Cost of Ownership)</strong>: Custo total de propriedade</li></ul>
<ul><li><strong>MVP (Minimum Viable Product)</strong>: Produto mÃ­nimo viÃ¡vel</li></ul>
<ul><li><strong>POC (Proof of Concept)</strong>: Prova de conceito</li></ul>
<ul><li><strong>TTM (Time to Market)</strong>: Tempo para chegar ao mercado</li></ul></p><p>---</p><p><h2>ğŸ—ºï¸ Jornada do UsuÃ¡rio e Fluxos Visuais</h2></p><p><h3>Fluxo Principal do UsuÃ¡rio</h3></p><p><pre><code>mermaid
flowchart TD
    A[UsuÃ¡rio acessa Dashboard] --> B{Autenticado?}
    B -->|NÃ£o| C[Login/MFA]
    B -->|Sim| D[Dashboard Home]
    C --> D
    
    D --> E[Interface Chat]
    E --> F[Digite/Fale Query]
    F --> G[Processamento LLM]
    
    G --> H{Tipo de Query?}
    H -->|Simples| I[Ollama Local]
    H -->|Complexa| J[Cloud LLM]
    H -->|HÃ­brida| K[Local + Cloud]
    
    I --> L[Resposta <500ms]
    J --> M[Resposta <3s]
    K --> N[Resposta Otimizada]
    
    L --> O[Gerar VisualizaÃ§Ã£o]
    M --> O
    N --> O
    
    O --> P[Exibir Resultado]
    P --> Q{Satisfeito?}
    Q -->|NÃ£o| R[Refinar Query]
    Q -->|Sim| S[Exportar/Compartilhar]
    R --> F
    S --> T[Finalizar SessÃ£o]
</code></pre></p><p><h3>Fluxo de Processamento LLM</h3></p><p><pre><code>mermaid
sequenceDiagram
    participant U as UsuÃ¡rio
    participant F as Frontend
    participant G as LLM Gateway
    participant L as Ollama Local
    participant C as Cloud LLM
    participant DB as Database
    participant R as Redis Cache
    
    U->>F: Envia query
    F->>G: POST /api/llm/query
    
    G->>R: Verifica cache
    alt Cache Hit
        R-->>G: Resposta cached
        G-->>F: Retorna resultado
    else Cache Miss
        G->>G: Analisa complexidade
        
        alt Query Simples
            G->>L: Processa localmente
            L-->>G: Resposta
        else Query Complexa
            G->>C: Envia para cloud
            C-->>G: Resposta
        else Query HÃ­brida
            par Processamento Paralelo
                G->>L: Contexto local
                G->>C: AnÃ¡lise complexa
            end
            L-->>G: Contexto
            C-->>G: Insights
            G->>G: Combina resultados
        end
        
        G->>R: Salva no cache
        G->>DB: Salva histÃ³rico
        G-->>F: Resposta final
    end
    
    F->>F: Gera visualizaÃ§Ã£o
    F-->>U: Exibe resultado
</code></pre></p><p><h3>Arquitetura de DecisÃ£o LLM</h3></p><p><pre><code>mermaid
flowchart TD
    A[Query Input] --> B[AnÃ¡lise de Complexidade]
    
    B --> C{AnÃ¡lise de Tipo}
    C -->|AgregaÃ§Ã£o Simples| D[Score: 1-3]
    C -->|Filtros BÃ¡sicos| E[Score: 1-2]
    C -->|ComparaÃ§Ãµes| F[Score: 2-4]
    C -->|PrevisÃµes| G[Score: 4-5]
    C -->|AnÃ¡lise Multi-dimensional| H[Score: 4-5]
    
    D --> I{Score â‰¤ 2?}
    E --> I
    F --> I
    G --> I
    H --> I
    
    I -->|Sim| J[Ollama Local]
    I -->|NÃ£o| K{Score â‰¤ 4?}
    
    K -->|Sim| L[HÃ­brido: Local + Cloud]
    K -->|NÃ£o| M[Cloud LLM Only]
    
    J --> N[Cache SemÃ¢ntico]
    L --> N
    M --> N
    
    N --> O[Resposta Final]
</code></pre></p><p>---</p><p><h2>âœ… CritÃ©rios de AceitaÃ§Ã£o por Funcionalidade</h2></p><p><h3>Feature 1: Interface Conversacional</h3>
#### CritÃ©rios de AceitaÃ§Ã£o
<ul><li>[ ] <strong>AC1.1</strong>: Sistema processa queries em portuguÃªs brasileiro</li></ul>
<ul><li>[ ] <strong>AC1.2</strong>: Interface suporta entrada por texto e voz</li></ul>
<ul><li>[ ] <strong>AC1.3</strong>: HistÃ³rico de conversas mantido por sessÃ£o</li></ul>
<ul><li>[ ] <strong>AC1.4</strong>: SugestÃµes contextuais aparecem durante digitaÃ§Ã£o</li></ul>
<ul><li>[ ] <strong>AC1.5</strong>: Tempo de resposta <500ms para queries locais</li></ul>
<ul><li>[ ] <strong>AC1.6</strong>: Fallback gracioso quando LLM local falha</li></ul>
<ul><li>[ ] <strong>AC1.7</strong>: MÃ¡ximo 3 tentativas de retry em caso de erro</li></ul></p><p>#### CenÃ¡rios de Teste
<pre><code>gherkin
CenÃ¡rio: Query simples de mÃ©tricas
  Dado que o usuÃ¡rio estÃ¡ logado no dashboard
  Quando ele digita "Quantas horas trabalhamos hoje?"
  EntÃ£o o sistema deve:
    - Processar via Ollama local
    - Retornar resposta em <500ms
    - Exibir grÃ¡fico apropriado
    - Salvar no histÃ³rico</p><p>CenÃ¡rio: Query por voz
  Dado que o usuÃ¡rio ativou o microfone
  Quando ele fala "Mostre a produtividade da equipe"
  EntÃ£o o sistema deve:
    - Converter voz para texto
    - Processar a query normalmente
    - Confirmar entendimento antes de executar
</code></pre></p><p><h3>Feature 2: Processamento HÃ­brido LLM</h3>
#### CritÃ©rios de AceitaÃ§Ã£o
<ul><li>[ ] <strong>AC2.1</strong>: Router decide automaticamente entre local/cloud/hÃ­brido</li></ul>
<ul><li>[ ] <strong>AC2.2</strong>: 70%+ das queries processadas localmente</li></ul>
<ul><li>[ ] <strong>AC2.3</strong>: Fallback para cloud quando local falha</li></ul>
<ul><li>[ ] <strong>AC2.4</strong>: Custo por query <$0.01 em mÃ©dia</li></ul>
<ul><li>[ ] <strong>AC2.5</strong>: Cache semÃ¢ntico com 85%+ hit rate</li></ul>
<ul><li>[ ] <strong>AC2.6</strong>: Logs detalhados de decisÃµes de roteamento</li></ul></p><p>#### CenÃ¡rios de Teste
<pre><code>gherkin
CenÃ¡rio: Query complexa requer cloud
  Dado uma query "Preveja quando terminaremos o sprint baseado no ritmo atual"
  Quando o sistema analisa a complexidade
  EntÃ£o deve:
    - Classificar como complexidade alta (score 4-5)
    - Rotear para cloud LLM
    - Registrar decisÃ£o nos logs
    - Processar em <3s</p><p>CenÃ¡rio: Economia de custos
  Dado 100 queries em um dia
  Quando analisamos o roteamento
  EntÃ£o:
    - â‰¥70 queries processadas localmente (custo $0)
    - â‰¤30 queries enviadas para cloud
    - Custo total <$0.50 por dia
</code></pre></p><p><h3>Feature 3: VisualizaÃ§Ãµes AutomÃ¡ticas</h3>
#### CritÃ©rios de AceitaÃ§Ã£o
<ul><li>[ ] <strong>AC3.1</strong>: IA escolhe tipo de grÃ¡fico baseado no contexto</li></ul>
<ul><li>[ ] <strong>AC3.2</strong>: Suporte para 8+ tipos de visualizaÃ§Ã£o (bar, line, pie, scatter, etc.)</li></ul>
<ul><li>[ ] <strong>AC3.3</strong>: GrÃ¡ficos responsivos em desktop e mobile</li></ul>
<ul><li>[ ] <strong>AC3.4</strong>: OpÃ§Ã£o de exportar em PNG, PDF, SVG</li></ul>
<ul><li>[ ] <strong>AC3.5</strong>: Drill-down interativo nos grÃ¡ficos</li></ul>
<ul><li>[ ] <strong>AC3.6</strong>: AtualizaÃ§Ã£o em tempo real via WebSocket</li></ul></p><p><h3>Feature 4: Sistema de Cache SemÃ¢ntico</h3>
#### CritÃ©rios de AceitaÃ§Ã£o
<ul><li>[ ] <strong>AC4.1</strong>: Queries similares retornam resultado cached</li></ul>
<ul><li>[ ] <strong>AC4.2</strong>: Similaridade calculada via embeddings</li></ul>
<ul><li>[ ] <strong>AC4.3</strong>: TTL configurÃ¡vel por tipo de query</li></ul>
<ul><li>[ ] <strong>AC4.4</strong>: Cache invalidado quando dados mudam</li></ul>
<ul><li>[ ] <strong>AC4.5</strong>: MÃ©tricas de cache hit/miss disponÃ­veis</li></ul>
<ul><li>[ ] <strong>AC4.6</strong>: Limpeza automÃ¡tica de cache antigo</li></ul></p><p>---</p><p><h2>ğŸ§ª EstratÃ©gia de Testes</h2></p><p><h3>PirÃ¢mide de Testes</h3>
<pre><code>
        /\
       /  \
      / E2E \ (10%)
     /______\
    /        \
   /Integration\ (20%)
  /_____________\
 /               \
/   Unit Tests    \ (70%)
\________________/
</code></pre></p><p><h3>Tipos de Teste</h3></p><p>#### 1. Testes UnitÃ¡rios (70%)
<pre><code>typescript
// Exemplo: Teste do LLM Router
describe('LLMRouter', () => {
  describe('analyzeComplexity', () => {
    it('should classify simple queries as local', () => {
      const query = "Quantas horas trabalhamos hoje?";
      const complexity = router.analyzeComplexity(query);
      expect(complexity.score).toBeLessThan(3);
      expect(complexity.route).toBe('local');
    });
    
    it('should classify predictions as cloud', () => {
      const query = "Quando vamos terminar o projeto?";
      const complexity = router.analyzeComplexity(query);
      expect(complexity.score).toBeGreaterThan(3);
      expect(complexity.route).toBe('cloud');
    });
  });
});
</code></pre></p><p>#### 2. Testes de IntegraÃ§Ã£o (20%)
<pre><code>typescript
describe('LLM Integration', () => {
  it('should process hybrid query correctly', async () => {
    const query = "Compare produtividade da equipe com mÃªs passado";
    const response = await llmService.processQuery(query);
    
    expect(response.processedBy).toBe('hybrid');
    expect(response.visualization).toBeDefined();
    expect(response.responseTime).toBeLessThan(3000);
  });
});
</code></pre></p><p>#### 3. Testes E2E (10%)
<pre><code>typescript
// Cypress E2E
describe('Dashboard Flow', () => {
  it('should complete full user journey', () => {
    cy.login();
    cy.visit('/dashboard');
    cy.get('[data-cy=chat-input]').type('MÃ©tricas de hoje');
    cy.get('[data-cy=send-button]').click();
    cy.get('[data-cy=chart-container]').should('be.visible');
    cy.get('[data-cy=response-text]').should('contain', 'horas');
  });
});
</code></pre></p><p><h3>Testes de Performance</h3>
<pre><code>yaml
Load Testing:
  Tool: Artillery.js
  Scenarios:
    - name: "LLM Query Load"
      weight: 70
      flow:
        - post:
            url: "/api/llm/query"
            json:
              query: "{{ $randomString() }}"
    
    - name: "Dashboard Metrics"
      weight: 30
      flow:
        - get:
            url: "/api/dashboard/metrics"
  
  Target:
    duration: 300
    arrivalRate: 10
    maxVusers: 100
  
  Assertions:
    - http.response_time.p95: 3000
    - http.response_time.p50: 500
    - http.codes.200: 95%
</code></pre></p><p><h3>Testes de LLM EspecÃ­ficos</h3>
<pre><code>python
<h1>Python - Teste de qualidade LLM</h1>
def test_llm_accuracy():
    test_cases = [
        {
            "query": "Quantas horas trabalhamos esta semana?",
            "expected_type": "aggregation",
            "expected_chart": "bar"
        },
        {
            "query": "Compare JoÃ£o e Maria",
            "expected_type": "comparison",
            "expected_chart": "comparison_bar"
        }
    ]
    
    for case in test_cases:
        response = llm_service.process(case["query"])
        assert response.type == case["expected_type"]
        assert response.visualization.type == case["expected_chart"]
        assert response.hallucination_score < 0.1
</code></pre></p><p>---</p><p><h2>ğŸ‘¥ Stakeholders</h2></p><p><h3>Stakeholder Map</h3>
<pre><code>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 STAKEHOLDERS MAP                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Grupo       â”‚   Papel     â”‚   Interesse          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sponsors      â”‚ C-Level     â”‚ ROI, Produtividade   â”‚
â”‚ Users         â”‚ Managers    â”‚ Insights, Facilidade â”‚
â”‚ Developers    â”‚ Time Dev    â”‚ Tracking, Fair Pay   â”‚
â”‚ Operations    â”‚ DevOps      â”‚ Estabilidade, Custos â”‚
â”‚ Security      â”‚ InfoSec     â”‚ Compliance, Privacy  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></p><p><h3>ComunicaÃ§Ã£o</h3>
<ul><li><strong>Weekly Status</strong>: Toda segunda-feira via Slack</li></ul>
<ul><li><strong>Sprint Reviews</strong>: Bi-semanais com demos</li></ul>
<ul><li><strong>Executive Reports</strong>: Mensais com KPIs</li></ul>
<ul><li><strong>User Feedback</strong>: Canal dedicado no Discord</li></ul></p><p>---</p><p><h2>ğŸ—ï¸ Arquitetura TÃ©cnica</h2></p><p><h3>VisÃ£o Geral - Arquitetura HÃ­brida</h3>
<pre><code>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FlowForge Dashboard                     â”‚
â”‚                Hybrid Architecture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Vue 3   â”‚â”€â”€â”€â”€â–¶â”‚ Node.js  â”‚â”€â”€â”€â”€â–¶â”‚   LLM    â”‚   â”‚
â”‚  â”‚ Frontend â”‚     â”‚ Backend  â”‚     â”‚ Gateway  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â–²                â–²                 â–²          â”‚
â”‚       â”‚                â”‚                 â”‚          â”‚
â”‚       â–¼                â–¼                 â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚WebSocket â”‚     â”‚PostgreSQLâ”‚     â”‚  Ollama  â”‚   â”‚
â”‚  â”‚Real-time â”‚     â”‚ Database â”‚     â”‚  (Local) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â–²                 â–²          â”‚
â”‚                        â”‚                 â”‚          â”‚
â”‚                        â–¼                 â–¼          â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                   â”‚  Redis   â”‚     â”‚Cloud LLM â”‚   â”‚
â”‚                   â”‚  Cache   â”‚     â”‚ (GPT-4)  â”‚   â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></p><p><h3>Stack TecnolÃ³gico</h3></p><p>#### Frontend
<ul><li><strong>Framework</strong>: Vue 3.5 + TypeScript 5.6</li></ul>
<ul><li><strong>Build Tool</strong>: Vite 5.4</li></ul>
<ul><li><strong>UI Library</strong>: PrimeVue 4.0</li></ul>
<ul><li><strong>Charts</strong>: Chart.js 4.4 + Apache ECharts 5.5</li></ul>
<ul><li><strong>State</strong>: Pinia 2.2</li></ul>
<ul><li><strong>Real-time</strong>: Socket.io-client 4.8</li></ul></p><p>#### Backend
<ul><li><strong>Runtime</strong>: Node.js 22 LTS</li></ul>
<ul><li><strong>Framework</strong>: Express 4.21 + TypeScript</li></ul>
<ul><li><strong>Database</strong>: PostgreSQL 16 + Prisma 6.0</li></ul>
<ul><li><strong>Cache</strong>: Redis 7.4</li></ul>
<ul><li><strong>Queue</strong>: Bull 4.16</li></ul>
<ul><li><strong>Auth</strong>: JWT + Passport.js</li></ul></p><p>#### LLM Integration
<ul><li><strong>Local</strong>: Ollama 0.5 (Llama 3.2, Mistral)</li></ul>
<ul><li><strong>Cloud</strong>: OpenAI GPT-4, Claude 3, Gemini</li></ul>
<ul><li><strong>Orchestration</strong>: LangChain.js 0.3</li></ul>
<ul><li><strong>Vector Store</strong>: ChromaDB</li></ul>
<ul><li><strong>Embeddings</strong>: text-embedding-3-small</li></ul></p><p><h3>Componentes Principais</h3></p><p>1. <strong>Chat Interface</strong>: Componente conversacional com histÃ³rico contextual
2. <strong>LLM Gateway</strong>: Router inteligente para seleÃ§Ã£o local vs cloud
3. <strong>Query Processor</strong>: Parser de linguagem natural e gerador de visualizaÃ§Ãµes
4. <strong>Cache System</strong>: Multi-camada (memory, Redis, CDN)
5. <strong>Real-time Engine</strong>: WebSocket para atualizaÃ§Ãµes ao vivo
6. <strong>Analytics Engine</strong>: AgregaÃ§Ãµes e cÃ¡lculos de mÃ©tricas</p><p>---</p><p><h2>ğŸ’¬ Funcionalidades com LLM</h2></p><p><h3>Interface Conversacional</h3></p><p>#### Exemplos de InteraÃ§Ã£o
<pre><code>
User: "Quantas horas trabalhamos esta semana?"
LLM: [GrÃ¡fico de barras] + "48 horas totais, mÃ©dia de 9.6h/dia"</p><p>User: "Compare produtividade entre JoÃ£o e Maria"
LLM: [GrÃ¡fico comparativo] + "JoÃ£o: 12 tickets, Maria: 8 tickets"</p><p>User: "Identifique gargalos no desenvolvimento"
LLM: [Timeline] + "3 gargalos: code review (2x tempo), 
      5 tickets parados, deploy manual (15% do tempo)"</p><p>User: "Preveja quando terminaremos o sprint"
LLM: [ProjeÃ§Ã£o] + "85% chance de conclusÃ£o atÃ© sexta-feira"
</code></pre></p><p><h3>Capacidades do Sistema</h3></p><p>#### Processamento Local (Ollama - Custo Zero)
<ul><li>Queries simples e diretas</li></ul>
<ul><li>AgregaÃ§Ãµes bÃ¡sicas</li></ul>
<ul><li>Filtros por data/pessoa</li></ul>
<ul><li>VisualizaÃ§Ãµes padrÃ£o</li></ul>
<ul><li>Tempo de resposta: <500ms</li></ul></p><p>#### Processamento Cloud (GPT-4/Claude - Premium)
<ul><li>AnÃ¡lises complexas multi-dimensionais</li></ul>
<ul><li>PrevisÃµes e projeÃ§Ãµes</li></ul>
<ul><li>DetecÃ§Ã£o de anomalias</li></ul>
<ul><li>RecomendaÃ§Ãµes estratÃ©gicas</li></ul>
<ul><li>Tempo de resposta: 1-3s</li></ul></p><p>#### Processamento HÃ­brido (Otimizado)
<ul><li>Local prÃ©-processa e extrai contexto</li></ul>
<ul><li>Cloud realiza anÃ¡lise profunda</li></ul>
<ul><li>Local pÃ³s-processa e formata</li></ul>
<ul><li>Melhor custo-benefÃ­cio</li></ul></p><p><h3>Features Especiais</h3></p><p>1. <strong>Voice-to-Query</strong>: Fale com o dashboard em portuguÃªs
2. <strong>Auto-VisualizaÃ§Ã£o</strong>: IA escolhe melhor tipo de grÃ¡fico
3. <strong>Insights Proativos</strong>: Alertas automÃ¡ticos de anomalias
4. <strong>Export Inteligente</strong>: RelatÃ³rios formatados automaticamente
5. <strong>Multi-LLM Support</strong>: Fallback entre providers
6. <strong>Cache SemÃ¢ntico</strong>: Queries similares usam cache</p><p>---</p><p><h2>ğŸ”Œ EspecificaÃ§Ãµes de API</h2></p><p><h3>RESTful Endpoints</h3></p><p><pre><code>yaml
Dashboard API:
  GET /api/dashboard/metrics
    - Description: MÃ©tricas gerais do dashboard
    - Auth: Bearer token
    - Response: MetricsResponse
  
  GET /api/dashboard/sessions
    - Description: SessÃµes de trabalho ativas
    - Auth: Bearer token
    - Response: SessionsResponse</p><p>LLM Gateway:
  POST /api/llm/query
    - Description: Processar query em linguagem natural
    - Body: { query: string, context?: object }
    - Response: { answer: string, visualization?: Chart }
  
  GET /api/llm/suggestions
    - Description: SugestÃµes contextuais
    - Response: string[]</p><p>WebSocket Events:
  connect: Estabelecer conexÃ£o
  metrics.update: AtualizaÃ§Ã£o de mÃ©tricas
  session.start: Nova sessÃ£o iniciada
  session.end: SessÃ£o finalizada
  llm.response: Resposta do LLM
</code></pre></p><p><h3>GraphQL Schema</h3></p><p><pre><code>graphql
type Query {
  dashboard: Dashboard!
  sessions(filter: SessionFilter): [Session!]!
  metrics(range: DateRange!): Metrics!
  llmQuery(input: String!): LLMResponse!
}</p><p>type Mutation {
  startSession(taskId: ID!): Session!
  endSession(sessionId: ID!): Session!
  sendLLMQuery(query: String!): LLMResponse!
}</p><p>type Subscription {
  metricsUpdate: Metrics!
  sessionUpdate: Session!
  llmStreaming(queryId: ID!): LLMStreamChunk!
}
</code></pre></p><p>---</p><p><h2>ğŸ—„ï¸ Modelagem de Dados</h2></p><p><h3>Esquema Principal (PostgreSQL)</h3></p><p><pre><code>sql
-- Tabela de Queries LLM
CREATE TABLE llm_queries (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id),
    query_text TEXT NOT NULL,
    query_embedding vector(1536),
    response_text TEXT,
    visualization_spec JSONB,
    provider VARCHAR(50), -- 'local', 'openai', 'anthropic'
    model_used VARCHAR(100),
    tokens_used INTEGER,
    cost_cents INTEGER,
    response_time_ms INTEGER,
    cache_hit BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW()
);</p><p>-- Tabela de MÃ©tricas
CREATE TABLE dashboard_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID REFERENCES projects(id),
    metric_type VARCHAR(50),
    metric_value NUMERIC,
    metadata JSONB,
    calculated_at TIMESTAMPTZ DEFAULT NOW()
);</p><p>-- Ãndices para Performance
CREATE INDEX idx_llm_queries_user_created 
    ON llm_queries(user_id, created_at DESC);
CREATE INDEX idx_llm_queries_embedding 
    ON llm_queries USING ivfflat (query_embedding vector_cosine_ops);
CREATE INDEX idx_metrics_project_type 
    ON dashboard_metrics(project_id, metric_type);
</code></pre></p><p><h3>Cache Structure (Redis)</h3></p><p><pre><code>javascript
// Cache Keys Pattern
{
  "dashboard:metrics:{projectId}": "2h TTL",
  "llm:query:{hash}": "24h TTL",
  "session:active:{userId}": "30min TTL",
  "suggestions:{context}": "1h TTL"
}
</code></pre></p><p><h3>Vector Store (ChromaDB)</h3></p><p><pre><code>python
<h1>Collections</h1>
collections = {
    "queries": {  # Historical queries for similarity
        "embedding_function": "text-embedding-3-small",
        "metadata": ["user_id", "timestamp", "cost"]
    },
    "documentation": {  # FlowForge docs for RAG
        "embedding_function": "text-embedding-3-small",
        "metadata": ["type", "version", "source"]
    }
}
</code></pre></p><p>---</p><p><h2>ğŸ“Š Plano de ImplementaÃ§Ã£o</h2></p><p><h3>Roadmap de 3 Meses</h3></p><p><pre><code>mermaid
gantt
    title FlowForge Dashboard LLM - Roadmap
    dateFormat YYYY-MM-DD
    
    section MÃªs 1 - MVP
    Setup Infraestrutura        :2025-01-01, 3d
    Frontend Base               :3d
    Backend Core                :4d
    LLM Integration BÃ¡sica      :4d
    Testes e Ajustes           :2d
    Deploy Beta                :1d
    
    section MÃªs 2 - ExpansÃ£o
    RAG Implementation         :7d
    Cache SemÃ¢ntico           :5d
    Ollama Local Setup        :5d
    Voice Interface           :3d
    
    section MÃªs 3 - Enterprise
    Fine-tuning Prep          :5d
    Model Training            :7d
    Advanced Analytics        :8d
    Production Deploy         :2d
</code></pre></p><p><h3>Milestones e Tarefas</h3></p><p>#### Milestone 1: MVP (2 semanas)
<ul><li>[ ] TASK-001: Setup Vue 3 + TypeScript (0.2h)</li></ul>
<ul><li>[ ] TASK-002: Configurar PrimeVue e tema (0.2h)</li></ul>
<ul><li>[ ] TASK-003: Backend Express scaffolding (0.2h)</li></ul>
<ul><li>[ ] TASK-004: PostgreSQL + Prisma setup (0.3h)</li></ul>
<ul><li>[ ] TASK-005: IntegraÃ§Ã£o GPT-4 bÃ¡sica (0.3h)</li></ul>
<ul><li>[ ] TASK-006: Chat interface simples (0.3h)</li></ul>
<ul><li>[ ] TASK-007: 5 queries prÃ©-definidas (0.3h)</li></ul>
<ul><li>[ ] TASK-008: Deploy em staging (0.2h)</li></ul></p><p>#### Milestone 2: HÃ­brido (1 mÃªs)
<ul><li>[ ] TASK-009: Instalar Ollama server (0.3h)</li></ul>
<ul><li>[ ] TASK-010: Router inteligente LLM (0.3h)</li></ul>
<ul><li>[ ] TASK-011: ChromaDB vector store (0.3h)</li></ul>
<ul><li>[ ] TASK-012: RAG pipeline (0.3h)</li></ul>
<ul><li>[ ] TASK-013: Cache semÃ¢ntico Redis (0.2h)</li></ul>
<ul><li>[ ] TASK-014: Voice-to-text PT-BR (0.3h)</li></ul>
<ul><li>[ ] TASK-015: VisualizaÃ§Ãµes dinÃ¢micas (0.3h)</li></ul>
<ul><li>[ ] TASK-016: Fallback system (0.2h)</li></ul></p><p>#### Milestone 3: Production (2-3 meses)
<ul><li>[ ] TASK-017: Dataset preparation (0.3h)</li></ul>
<ul><li>[ ] TASK-018: Fine-tuning GPT-3.5 (0.3h)</li></ul>
<ul><li>[ ] TASK-019: A/B testing framework (0.3h)</li></ul>
<ul><li>[ ] TASK-020: Advanced analytics (0.3h)</li></ul>
<ul><li>[ ] TASK-021: Anomaly detection (0.3h)</li></ul>
<ul><li>[ ] TASK-022: Cost optimization (0.2h)</li></ul>
<ul><li>[ ] TASK-023: Security hardening (0.3h)</li></ul>
<ul><li>[ ] TASK-024: Production deploy (0.2h)</li></ul></p><p>---</p><p><h2>ğŸ“‹ GestÃ£o de MudanÃ§a e Treinamento</h2></p><p><h3>EstratÃ©gia de AdoÃ§Ã£o</h3></p><p>#### Fase 1: Pioneiros (Semana 1-2)
<strong>PÃºblico</strong>: 2-3 early adopters tÃ©cnicos
<strong>Objetivo</strong>: Validar funcionalidades bÃ¡sicas e coletar feedback inicial
<strong>Atividades</strong>:
<ul><li>Workshop de 2h sobre funcionalidades</li></ul>
<ul><li>Uso supervisionado por 1 semana</li></ul>
<ul><li>SessÃµes de feedback diÃ¡rias</li></ul>
<ul><li>Ajustes baseados na experiÃªncia</li></ul></p><p>#### Fase 2: Grupo Piloto (Semana 3-6)
<strong>PÃºblico</strong>: 30% da equipe (managers e leads)
<strong>Objetivo</strong>: Testar em cenÃ¡rios reais e refinar UX
<strong>Atividades</strong>:
<ul><li>Treinamento de 1h para novos usuÃ¡rios</li></ul>
<ul><li>DocumentaÃ§Ã£o de casos de uso</li></ul>
<ul><li>Suporte dedicado via Slack</li></ul>
<ul><li>MÃ©tricas de adoÃ§Ã£o semanais</li></ul></p><p>#### Fase 3: Rollout Completo (Semana 7-12)
<strong>PÃºblico</strong>: 100% da organizaÃ§Ã£o
<strong>Objetivo</strong>: AdoÃ§Ã£o completa e autonomia
<strong>Atividades</strong>:
<ul><li>SessÃµes de treinamento em grupo (30min)</li></ul>
<ul><li>VÃ­deos tutoriais auto-explicativos</li></ul>
<ul><li>FAQ baseado em feedback real</li></ul>
<ul><li>Champions internos para suporte</li></ul></p><p><h3>Material de Treinamento</h3></p><p>#### Tutorial Interativo (In-App)
<pre><code>typescript
// Exemplo de tutorial guiado
const tutorialSteps = [
  {
    target: '.chat-input',
    title: 'Digite sua pergunta',
    content: 'FaÃ§a perguntas em portuguÃªs natural, como "Quantas horas trabalhei hoje?"',
    position: 'bottom'
  },
  {
    target: '.voice-button',
    title: 'Use sua voz',
    content: 'Clique aqui para falar diretamente com o dashboard',
    position: 'left'
  },
  {
    target: '.chart-container',
    title: 'VisualizaÃ§Ãµes automÃ¡ticas',
    content: 'O sistema gera grÃ¡ficos baseados na sua pergunta',
    position: 'top'
  }
];
</code></pre></p><p>#### VÃ­deos de Treinamento
1. <strong>"Primeiros Passos"</strong> (5min): Login, navegaÃ§Ã£o bÃ¡sica, primeira query
2. <strong>"Queries AvanÃ§adas"</strong> (8min): ComparaÃ§Ãµes, filtros, previsÃµes
3. <strong>"Interface de Voz"</strong> (3min): Como usar comandos de voz efetivamente
4. <strong>"Exportando Dados"</strong> (4min): Salvando grÃ¡ficos e relatÃ³rios</p><p>#### Guia de ReferÃªncia RÃ¡pida
<pre><code>markdown
<h1>Quick Reference - Perguntas Comuns</h1></p><p><h2>MÃ©tricas BÃ¡sicas</h2>
<ul><li>"Horas trabalhadas hoje/semana/mÃªs"</li></ul>
<ul><li>"Quantos tickets fechamos?"</li></ul>
<ul><li>"Qual o status do sprint atual?"</li></ul></p><p><h2>ComparaÃ§Ãµes</h2>
<ul><li>"Compare [pessoa A] com [pessoa B]"</li></ul>
<ul><li>"Esta semana vs semana passada"</li></ul>
<ul><li>"Performance por projeto"</li></ul></p><p><h2>PrevisÃµes</h2>
<ul><li>"Quando vamos terminar o sprint?"</li></ul>
<ul><li>"Vamos cumprir o prazo?"</li></ul>
<ul><li>"Quantos tickets podemos fazer?"</li></ul></p><p><h2>Troubleshooting</h2>
<ul><li>Se nÃ£o entender: Seja mais especÃ­fico</li></ul>
<ul><li>Erro de dados: Verifique se sessions estÃ£o rodando</li></ul>
<ul><li>Resposta lenta: Query complexa foi para cloud (normal)</li></ul>
</code></pre></p><p><h3>Plano de ComunicaÃ§Ã£o</h3></p><p>#### PrÃ©-LanÃ§amento (2 semanas antes)
<ul><li><strong>Email executivo</strong>: AnÃºncio oficial do projeto</li></ul>
<ul><li><strong>All-hands meeting</strong>: ApresentaÃ§Ã£o de 15min sobre benefÃ­cios</li></ul>
<ul><li><strong>Slack channel</strong>: CriaÃ§Ã£o de #flowforge-dashboard-beta</li></ul></p><p>#### Durante Rollout (Semanas 1-12)
<ul><li><strong>Weekly updates</strong>: Status de adoÃ§Ã£o e novidades</li></ul>
<ul><li><strong>Success stories</strong>: Cases de uso que geraram valor</li></ul>
<ul><li><strong>Feature highlights</strong>: Showcases de funcionalidades</li></ul></p><p>#### PÃ³s-LanÃ§amento (ContÃ­nuo)
<ul><li><strong>Monthly metrics</strong>: KPIs de adoÃ§Ã£o e satisfaÃ§Ã£o</li></ul>
<ul><li><strong>Feature requests</strong>: Canal para sugestÃµes da comunidade</li></ul>
<ul><li><strong>Best practices</strong>: Compartilhamento de queries Ãºteis</li></ul></p><p><h3>MÃ©tricas de AdoÃ§Ã£o</h3></p><p><pre><code>yaml
AdoÃ§Ã£o por Fase:
  Fase 1 (Pioneiros):
    - UsuÃ¡rios ativos: 100%
    - Queries por dia: >10
    - NPS: >7
  
  Fase 2 (Piloto):
    - UsuÃ¡rios ativos: >80%
    - Queries por usuÃ¡rio/dia: >3
    - Tickets de suporte: <2/semana
  
  Fase 3 (Rollout):
    - UsuÃ¡rios ativos: >90%
    - SubstituiÃ§Ã£o de relatÃ³rios manuais: >50%
    - ROI demonstrado: 3 meses
</code></pre></p><p>---</p><p><h2>ğŸ’° AnÃ¡lise de Custos</h2></p><p><h3>Custos de Desenvolvimento</h3>
| Item | Horas | Custo/Hora | Total |
|------|-------|------------|-------|
| <strong>Frontend</strong> | 40h | $150 | $6,000 |
| <strong>Backend</strong> | 60h | $150 | $9,000 |
| <strong>LLM Integration</strong> | 40h | $200 | $8,000 |
| <strong>Testing</strong> | 20h | $120 | $2,400 |
| <strong>DevOps</strong> | 20h | $180 | $3,600 |
| <strong>Total</strong> | 180h | - | <strong>$29,000</strong> |</p><p><h3>Custos Operacionais (Mensal)</h3>
| Componente | Uso Baixo | Uso MÃ©dio | Uso Alto |
|------------|-----------|-----------|----------|
| <strong>Cloud LLM (30%)</strong> | $30 | $100 | $300 |
| <strong>Infraestrutura</strong> | $100 | $200 | $500 |
| <strong>Ollama Server</strong> | $50 | $50 | $100 |
| <strong>Total Mensal</strong> | <strong>$180</strong> | <strong>$350</strong> | <strong>$900</strong> |</p><p><h3>ROI Esperado</h3>
<ul><li><strong>Economia em RelatÃ³rios</strong>: 10h/semana Ã— $100/h = $4,000/mÃªs</li></ul>
<ul><li><strong>ReduÃ§Ã£o de Custos LLM</strong>: 60-70% vs cloud-only = $200-500/mÃªs</li></ul>
<ul><li><strong>Aumento de Produtividade</strong>: 20% = ~$8,000/mÃªs valor agregado</li></ul>
<ul><li><strong>Payback</strong>: 2-3 meses</li></ul></p><p>---</p><p><h2>ğŸ”’ SeguranÃ§a e Compliance</h2></p><p><h3>Medidas de SeguranÃ§a</h3>
1. <strong>AutenticaÃ§Ã£o</strong>: JWT com refresh tokens, MFA opcional
2. <strong>AutorizaÃ§Ã£o</strong>: RBAC com polÃ­ticas granulares
3. <strong>Criptografia</strong>: TLS 1.3 em trÃ¢nsito, AES-256 em repouso
4. <strong>SanitizaÃ§Ã£o</strong>: Input validation, output encoding
5. <strong>Rate Limiting</strong>: Por endpoint e por usuÃ¡rio
6. <strong>Audit Trail</strong>: Log completo de todas aÃ§Ãµes</p><p><h3>Privacidade de Dados</h3>
<ul><li><strong>LGPD/GDPR Compliance</strong>: Direito ao esquecimento, portabilidade</li></ul>
<ul><li><strong>Data Minimization</strong>: Apenas dados necessÃ¡rios</li></ul>
<ul><li><strong>Anonymization</strong>: PII removido antes do LLM</li></ul>
<ul><li><strong>Local Processing</strong>: Dados sensÃ­veis nunca saem do servidor</li></ul></p><p><h3>LLM Security</h3>
<ul><li><strong>Prompt Injection Prevention</strong>: Input sanitization</li></ul>
<ul><li><strong>Token Limits</strong>: MÃ¡ximo de tokens por request</li></ul>
<ul><li><strong>Cost Thresholds</strong>: Alertas de gastos anormais</li></ul>
<ul><li><strong>Output Validation</strong>: VerificaÃ§Ã£o de respostas</li></ul></p><p>---</p><p><h2>âš ï¸ Riscos e MitigaÃ§Ãµes</h2></p><p><h3>Riscos TÃ©cnicos</h3>
| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| <strong>LatÃªncia do LLM</strong> | MÃ©dia | Alto | Cache agressivo, timeouts |
| <strong>Custos de API</strong> | Alta | MÃ©dio | HÃ­brido local/cloud |
| <strong>Falha do Ollama</strong> | Baixa | MÃ©dio | Fallback para cloud |
| <strong>Volume de dados</strong> | MÃ©dia | MÃ©dio | PaginaÃ§Ã£o, lazy loading |</p><p><h3>Riscos de NegÃ³cio</h3>
| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| <strong>Baixa adoÃ§Ã£o</strong> | MÃ©dia | Alto | Training, UX simples |
| <strong>ROI nÃ£o atingido</strong> | Baixa | Alto | MÃ©tricas claras, iteraÃ§Ã£o |
| <strong>Compliance issues</strong> | Baixa | Alto | Audit regular, LGPD |</p><p>---</p><p><h2>ğŸ“ˆ MÃ©tricas de Sucesso</h2></p><p><h3>Technical Metrics</h3>
<ul><li><strong>Response Time</strong>: p95 < 500ms (local), < 3s (cloud)</li></ul>
<ul><li><strong>Availability</strong>: 99.9% uptime</li></ul>
<ul><li><strong>Error Rate</strong>: < 0.1%</li></ul>
<ul><li><strong>Cache Hit Ratio</strong>: > 85%</li></ul>
<ul><li><strong>Query Success Rate</strong>: > 95%</li></ul></p><p><h3>Business Metrics</h3>
<ul><li><strong>User Adoption</strong>: 100% em 3 meses</li></ul>
<ul><li><strong>Time Saved</strong>: 10h/semana em relatÃ³rios</li></ul>
<ul><li><strong>Cost Reduction</strong>: 60% em APIs LLM</li></ul>
<ul><li><strong>User Satisfaction</strong>: NPS > 50</li></ul>
<ul><li><strong>ROI</strong>: Positivo em 3 meses</li></ul></p><p><h3>LLM Specific Metrics</h3>
<ul><li><strong>Query Accuracy</strong>: > 90%</li></ul>
<ul><li><strong>Hallucination Rate</strong>: < 5%</li></ul>
<ul><li><strong>Local Processing</strong>: > 70% queries</li></ul>
<ul><li><strong>Average Cost/Query</strong>: < $0.01</li></ul>
<ul><li><strong>Context Relevance</strong>: > 85%</li></ul></p><p>---</p><p><h2>ğŸ“„ ApÃªndice: Technical Decision Records (TDRs)</h2></p><p><h3>TDR-001: Arquitetura HÃ­brida LLM</h3></p><p><strong>Data</strong>: 2025-09-12  
<strong>Status</strong>: Approved  
<strong>Contexto</strong>: Precisamos balancear custo e qualidade no processamento de queries</p><p><strong>DecisÃ£o</strong>: Implementar arquitetura hÃ­brida com Ollama local + Cloud LLM</p><p><strong>OpÃ§Ãµes Consideradas</strong>:
1. <strong>Cloud-only</strong> (GPT-4 para tudo)
   - Pros: Qualidade mÃ¡xima, simplicidade
   - Cons: Custo alto ($500-2000/mÃªs)</p><p>2. <strong>Local-only</strong> (Ollama apenas)
   - Pros: Custo zero, privacidade
   - Cons: LimitaÃ§Ãµes em anÃ¡lises complexas</p><p>3. <strong>HÃ­brida</strong> (Local + Cloud inteligente) âœ…
   - Pros: 60-70% economia, qualidade mantida
   - Cons: Complexidade arquitetural</p><p><strong>ConsequÃªncias</strong>:
<ul><li>Router inteligente necessÃ¡rio</li></ul>
<ul><li>Sistema de fallback requerido</li></ul>
<ul><li>Monitoramento de custos crÃ­tico</li></ul>
<ul><li>Potencial economia de $300-1500/mÃªs</li></ul></p><p><h3>TDR-002: Frontend Framework Selection</h3></p><p><strong>Data</strong>: 2025-09-12  
<strong>Status</strong>: Approved  
<strong>Contexto</strong>: Escolha do framework para interface do dashboard</p><p><strong>DecisÃ£o</strong>: Vue 3 + TypeScript + PrimeVue</p><p><strong>OpÃ§Ãµes Consideradas</strong>:
1. <strong>React + TypeScript</strong>
   - Pros: Ecossistema grande, skills existentes
   - Cons: Overhead, complexidade</p><p>2. <strong>Vue 3 + TypeScript</strong> âœ…
   - Pros: Simplicidade, performance, Composition API
   - Cons: Menor ecosistema que React</p><p>3. <strong>Svelte + TypeScript</strong>
   - Pros: Performance mÃ¡xima, simplicidade
   - Cons: Ecosystem limitado, skills gap</p><p><strong>ConsequÃªncias</strong>:
<ul><li>Curva de aprendizado mÃ­nima</li></ul>
<ul><li>PrimeVue fornece componentes UI prontos</li></ul>
<ul><li>TypeScript garante type safety</li></ul>
<ul><li>Desenvolvimento mais rÃ¡pido</li></ul></p><p><h3>TDR-003: Vector Database Selection</h3></p><p><strong>Data</strong>: 2025-09-12  
<strong>Status</strong>: Approved  
<strong>Contexto</strong>: Cache semÃ¢ntico requer busca por similaridade</p><p><strong>DecisÃ£o</strong>: ChromaDB para vector store</p><p><strong>OpÃ§Ãµes Consideradas</strong>:
1. <strong>Pinecone</strong>
   - Pros: Managed service, escalabilidade
   - Cons: Custo, vendor lock-in</p><p>2. <strong>ChromaDB</strong> âœ…
   - Pros: Open source, local, Python integraÃ§Ã£o
   - Cons: Menos features enterprise</p><p>3. <strong>Weaviate</strong>
   - Pros: GraphQL, features avanÃ§adas
   - Cons: Complexidade, overhead</p><p><strong>ConsequÃªncias</strong>:
<ul><li>Self-hosted reduz custos</li></ul>
<ul><li>IntegraÃ§Ã£o com pipeline Python</li></ul>
<ul><li>Controle total sobre dados</li></ul>
<ul><li>PossÃ­vel migraÃ§Ã£o futura para managed</li></ul></p><p><h3>TDR-004: Cache Strategy</h3></p><p><strong>Data</strong>: 2025-09-12  
<strong>Status</strong>: Approved  
<strong>Contexto</strong>: OtimizaÃ§Ã£o de performance e custos LLM</p><p><strong>DecisÃ£o</strong>: Multi-layer cache (Memory + Redis + Semantic)</p><p><strong>OpÃ§Ãµes Consideradas</strong>:
1. <strong>Redis simples</strong>
   - Pros: Simplicidade
   - Cons: Miss em queries similares</p><p>2. <strong>Cache semÃ¢ntico only</strong>
   - Pros: Inteligente
   - Cons: Overhead computacional</p><p>3. <strong>Multi-layer</strong> âœ…
   - Pros: Melhor hit rate, performance
   - Cons: Complexidade gerencial</p><p><strong>ConsequÃªncias</strong>:
<ul><li>L1: Memory cache (100ms)</li></ul>
<ul><li>L2: Redis cache (10ms)</li></ul>
<ul><li>L3: Semantic cache (ChromaDB)</li></ul>
<ul><li>Target: 85%+ hit rate</li></ul></p><p>---</p><p><h2>ğŸ”„ Controle de VersÃ£o e ManutenÃ§Ã£o</h2></p><p><h3>Versionamento do Documento</h3></p><p>| VersÃ£o | Data | Autor | MudanÃ§as |
|--------|------|-------|----------|
| 1.0.0 | 2025-09-12 | FlowForge Maestro | VersÃ£o inicial |
| 2.0.0 | 2025-09-12 | FFT-Documentation | Enhanced com todas seÃ§Ãµes |</p><p><h3>Processo de RevisÃ£o</h3></p><p>#### RevisÃµes ObrigatÃ³rias
<ul><li><strong>Semanal</strong>: Durante desenvolvimento ativo</li></ul>
<ul><li><strong>Mensal</strong>: Durante operaÃ§Ã£o normal</li></ul>
<ul><li><strong>Ad-hoc</strong>: Quando mudanÃ§as significativas ocorrem</li></ul></p><p>#### CritÃ©rios de AtualizaÃ§Ã£o
<pre><code>yaml
Triggers para atualizaÃ§Ã£o:
  - MudanÃ§a na arquitetura: Major version bump
  - Novos features: Minor version bump
  - CorreÃ§Ãµes/clarificaÃ§Ãµes: Patch version bump
  - Feedback de stakeholders: Review necessÃ¡ria
  - MudanÃ§as regulatÃ³rias: Immediate update
</code></pre></p><p>#### Processo de Approval
1. <strong>Autor</strong> cria draft das mudanÃ§as
2. <strong>Tech Lead</strong> revisa aspectos tÃ©cnicos
3. <strong>Product Owner</strong> revisa requisitos de negÃ³cio
4. <strong>Security</strong> revisa mudanÃ§as de seguranÃ§a (se aplicÃ¡vel)
5. <strong>Stakeholders</strong> aprovaÃ§Ã£o final
6. <strong>PublicaÃ§Ã£o</strong> e comunicaÃ§Ã£o das mudanÃ§as</p><p><h3>ManutenÃ§Ã£o da DocumentaÃ§Ã£o</h3></p><p>#### Responsabilidades
<ul><li><strong>Product Owner</strong>: MantÃ©m requisitos de negÃ³cio atualizados</li></ul>
<ul><li><strong>Tech Lead</strong>: Garante precisÃ£o tÃ©cnica</li></ul>
<ul><li><strong>Documentation Lead</strong>: Coordena processo de atualizaÃ§Ã£o</li></ul>
<ul><li><strong>Team Members</strong>: Reportam inconsistÃªncias/gaps</li></ul></p><p>#### MÃ©tricas de Qualidade
<pre><code>yaml
Quality Gates:
  - Links funcionais: 100%
  - SeÃ§Ãµes desatualizadas: 0
  - Feedback nÃ£o endereÃ§ado: <7 dias
  - Reviews em atraso: 0
  - Stakeholder NPS: >4.0/5.0
</code></pre></p><p>---</p><p><h2>ğŸš€ PrÃ³ximos Passos</h2></p><p><h3>Imediato (Semana 1)</h3>
1. âœ… Aprovar PRD e arquitetura
2. ğŸ”„ Criar issues no GitHub (#468-#494)
3. ğŸ“‹ Setup ambiente de desenvolvimento
4. ğŸ—ï¸ Iniciar implementaÃ§Ã£o do MVP</p><p><h3>Curto Prazo (MÃªs 1)</h3>
1. ğŸ¯ MVP funcional com queries bÃ¡sicas
2. ğŸ§ª Testes com usuÃ¡rios beta
3. ğŸ“Š Coletar mÃ©tricas iniciais
4. ğŸ”„ IteraÃ§Ã£o baseada em feedback</p><p><h3>MÃ©dio Prazo (Meses 2-3)</h3>
1. ğŸ¤– Implementar processamento hÃ­brido
2. ğŸ¤ Adicionar interface de voz
3. ğŸ“ˆ Fine-tuning com dados reais
4. ğŸš€ Deploy em produÃ§Ã£o</p><p>---</p><p><h2>ğŸ“ Contatos</h2></p><p><h3>Time Core</h3>
<ul><li><strong>Product Owner</strong>: [Alex Cruz]</li></ul>
<ul><li><strong>Tech Lead</strong>: [TBD]</li></ul>
<ul><li><strong>LLM Specialist</strong>: [TBD]</li></ul>
<ul><li><strong>DevOps Lead</strong>: [TBD]</li></ul></p><p><h3>Canais de ComunicaÃ§Ã£o</h3>
<ul><li><strong>Slack</strong>: #flowforge-dashboard</li></ul>
<ul><li><strong>GitHub</strong>: github.com/JustCode-CruzAlex/FlowForge</li></ul>
<ul><li><strong>Documentation</strong>: /documentation/2.0/dashboard/</li></ul></p><p>---</p><p><h2>ğŸ“ Anexos</h2></p><p><h3>Documentos Relacionados</h3>
1. [Arquitetura TÃ©cnica Detalhada](./architecture/technical_spec.md)
2. [API Specification](./api/FLOWFORGE_DASHBOARD_API_SPEC.md)
3. [Database Schema](./database/schema/)
4. [Test Plan](./testing/test_plan.md)
5. [Security Assessment](./security/assessment.md)</p><p><h3>ReferÃªncias</h3>
<ul><li>[FlowForge Documentation](https://flowforge.io/docs)</li></ul>
<ul><li>[Ollama Documentation](https://ollama.ai/docs)</li></ul>
<ul><li>[LangChain.js Guide](https://js.langchain.com)</li></ul>
<ul><li>[Vue 3 Best Practices](https://vuejs.org/guide)</li></ul></p><p>---</p><p><strong>Documento gerado pelo FlowForge Maestro System</strong>  
<strong>VersÃ£o</strong>: 2.0.0  
<strong>Data</strong>: 2025-09-12  
<strong>Status</strong>: Aguardando AprovaÃ§Ã£o</p><p>---</p><p><h2>ğŸ¯ Call to Action</h2></p><p><strong>Para aprovar este PRD e iniciar o desenvolvimento:</strong></p><p><pre><code>bash
<h1>Aprovar e criar issues</h1>
/flowforge:session:start dashboard-llm</p><p><h1>Ou revisar alteraÃ§Ãµes necessÃ¡rias</h1>
<h1>Informe quais ajustes sÃ£o necessÃ¡rios</h1>
</code></pre></p><p>Este PRD foi criado atravÃ©s da orquestraÃ§Ã£o de mÃºltiplos agentes especialistas FlowForge, garantindo completude tÃ©cnica e alinhamento com os objetivos de negÃ³cio.</p>
    <div style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ecf0f1; text-align: center; color: #7f8c8d;">
        <p>FlowForge Dashboard LLM Integration - Product Requirements Document</p>
        <p>Generated: 2025-09-12 | Version: 1.0.0</p>
    </div>
</body>
</html>